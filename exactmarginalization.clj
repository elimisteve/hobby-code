;; Chapter 24 of Information Theory, Inference and Learning Algorithms

;; We have some data which we imagine comes from a gaussian distribution.
;; We're trying to calculate various best guesses for mu and sigma 

;; First we need a definition of the gaussian distribution function:
(defn gaussian [mu sigma]
  (let [factor (/ 1 (Math/sqrt (* 2 Math/PI)) sigma)]
    (fn [x]
      (let [ssd (/ (- x mu) sigma)
            sd (* ssd ssd)]
        (* factor (Math/exp (* -1/2 sd)))))))

;; And a method of integration. This is Simpson's rule, which calculates the area of
;; a load of parallelograms fitted under the curve
(defn integrate [f a b N]
  (let [h (/ (- b a) N)
        points (map #(+ a (* % h)) (range (inc N)))
        vals (map f points)
        simpson-coefficients (concat [1] (repeat (dec N) 2) [1])
        simpson (map * simpson-coefficients vals)]
    (* (/ h 2) (reduce + simpson))))

;; integral of x over 0 1 is 1/2. Simpson's rule should make it exact.
(integrate identity 0 1 1) ; 1/2
(integrate identity 0 1 10) ; 1/2
;; of x^2 is 1/3, Should be an h^2 approximation
(integrate #(* % %) 0 1 1) ; 1/2
(integrate #(* % %) 0 1 10) ; 67/200
(integrate #(* % %) 0 1 100) ; 6667/20000
(integrate #(* % %) 0 1 1000) ; 666667/2000000
(integrate #(* % %) 0 1 10000) ; 66666667/200000000
(integrate #(* % %) 0 1 10000.0) ; 0.33333333499999834


;; sanity checks. A gaussian with mean 0 and standard deviation 1 should have a peak at 0
(map (gaussian 0 1) [-0.01 0 0.01]) ; (0.39892233378608216 0.3989422804014327 0.39892233378608216)
;; should have a total probability mass of 1
(integrate (gaussian 0 1) -100 100 10000) ; 0.9999999999999989
;; and the majority of its mass should be between -1 and 1
(integrate (gaussian 0 1) -1 1 10000) ; 0.6826894905239459
;; 95 % of the probability mass should within 2 standard deviations
(integrate (gaussian 0 1) -2 2 1000) ; 0.9544994481518969
;; Similarly, with mean 10 and sd 5
(integrate (gaussian 10 5) -100 100 10000) ; 0.9999999999999988
(integrate (gaussian 10 5) 5 15 10000) ; 0.6826894905239471
(integrate (gaussian 10 5) 0 20 10000) ; 0.9544997332241237

;; Given our gaussian function, we can calculate the probability of a data set
;; xs being generated by a gaussian of mean mu and sd sigma, which, when
;; considered as a function of mu and sigma for a fixed data set, is known as
;; the likelihood
(defn likelihood [xs mu sigma]
  (reduce * (map (gaussian mu sigma) xs)))

;; One classical way to estimate the parameters mu and sigma is to take the
;; parameters which maximize the likelihood.
;; In order not to introduce any magic functions, I'll do the maximization by hand.
(likelihood [-1 0 1] 0 1) ; 0.02335800330543158
(likelihood [-1 0 1] 0 2) ; 0.006181111673204696
(likelihood [-1 0 1] 0 0.1) ; 2.362011496691831E-42
(likelihood [-1 0 1] 0 0.4) ; 0.001915180501771747
(likelihood [-1 0 1] 0 0.5) ; 0.009303412060034514
(likelihood [-1 0 1] 0 0.6) ; 0.018276914721837227
(likelihood [-1 0 1] 0 0.7) ; 0.024050317175942977
(likelihood [-1 0 1] 0 0.8) ; 0.025994119342662193
(likelihood [-1 0 1] 0 0.9) ; 0.025341752327009883
(likelihood [-1 0 1] 0 0.81) ; 0.026022066004237898
(likelihood [-1 0 1] 0 0.82) ; 0.02602564769190768
(likelihood [-1 0 1] 0 0.83) ; 0.026006302784975496
(likelihood [-1 0 1] 0 0.811) ; 0.02602349662698512
(likelihood [-1 0 1] 0 0.812) ; 0.026024685075132118
(likelihood [-1 0 1] 0 0.813) ; 0.026025632798181965
(likelihood [-1 0 1] 0 0.814) ; 0.02602634124274292
(likelihood [-1 0 1] 0 0.815) ; 0.026026811852466962
(likelihood [-1 0 1] 0 0.816) ; 0.026027046067989387
(likelihood [-1 0 1] 0 0.817) ; 0.026027045326869728



;; Directly calculating we're playing off sigma against average of residuals squared
;;we want mu=0 and sigma = sqrt(2/3)
(Math/sqrt 2/3) ; 0.816496580927726

;; Which convinces me that my gaussian formula's right!

;; sanity checks for the likelihood function
;; likelihood of a gaussian with sd sigma hitting its mean n times should be
;; (in latex notation) {\sqrt{(1/2pi)}\sigma}^n
(/ (Math/pow (Math/sqrt (* Math/PI 2)) 3)) ; 0.06349363593424098
(likelihood [0 0 0] 0 1) ; 0.06349363593424098
(likelihood [100 100 100] 100 1) ; 0.06349363593424098
;; As sigma reduces, it should blow up violently
(likelihood [0 0 0] 0 1) ; 63.49363593424098
(likelihood [0 0 0] 0 0.1) ; 63.49363593424098
(likelihood [0 0 0] 0 0.01) ; 63493.63593424098
;; More violently as the number of data points increases
(likelihood [0 0 0 0 0] 0 1) ; 0.010105326013811646
(likelihood [0 0 0 0 0] 0 0.1) ; 1010.5326013811645
(likelihood [0 0 0 0 0] 0 0.01) ; 1.0105326013811643E8
;; but if the data points are spread out
;; then the maximum likelihood should be at their mean for any given sigma
(likelihood [1 2 3 4 5] 2.9 1) ; 6.640802395922553E-5
(likelihood [1 2 3 4 5] 3 1) ; 6.808915108954251E-5
(likelihood [1 2 3 4 5] 3.1 1) ; 6.640802395922553E-5

;; and the global maximum likelihood should have mean equal to the sample mean
;; and with a standard deviation which is the square root of the sample variance
(Math/sqrt (/ (reduce + (map #(* % %) (map #(- % 3) [1 2 3 4 5]))) 5)) ; 1.4142135623730951
(likelihood [1 2 3 4 5] 3 1.4142) ; 1.4663550358059296E-4

;; This is good evidence that that's true:
(likelihood [1 2 3 4 5] 2.99 1.4142) ; 1.466171749366661E-4
(likelihood [1 2 3 4 5] 3.01 1.4142) ; 1.466171749366661E-4
(likelihood [1 2 3 4 5] 3 1.40) ; 1.4656020514688226E-4
(likelihood [1 2 3 4 5] 3 1.42) ; 1.4662331295862982E-4






;; What about the a priori?

;; An improper prior flat in mu and in log sigma
(defn prior [mu sigma]
                                        ;(/ sigma)
  1)


(defn apriori [xs mu sigma]
  (* (likelihood xs mu sigma) (prior mu sigma)))

;; to find P(xs|sigma) we want to marginalize over mu

;; Approximate the integral over all mu with
(defn marginal [xs sigma]
  (reduce + (map #(apriori xs % sigma) (range -5 5 0.01))))

(marginal [-1 0 1] 0.1) ; 3.418308964574561E-41
(marginal [-1 0 1] 1) ; 3.3803760991572847
(marginal [-1 0 1] 10) ; 0.0558578109202383
(marginal [-1 0 1] 2) ; 1.789037924279182
(marginal [-1 0 1] 0.5) ; 0.6731960638313484
(marginal [-1 0 1] 0.4) ; 0.11086616110300546
(marginal [-1 0 1] 0.6) ; 1.5870238106611063
(marginal [-1 0 1] 0.7) ; 2.4363975553451214
(marginal [-1 0 1] 0.8) ; 3.0095003787980272
(marginal [-1 0 1] 0.8) ; 3.0095003787980272
(marginal [-1 0 1] 0.9) ; 3.300718279775264
(marginal [-1 0 1] 0.81) ; 3.0503951363120567
(marginal [-1 0 1] 0.79) ; 2.9657332974866253
(marginal [-1 0 1] 0.82) ; 3.088479376571154
(marginal [-1 0 1] 0.83) ; 3.123820091390652
(marginal [-1 0 1] 0.815) ; 3.06978449450132
(marginal [-1 0 1] 1) ; 3.3803760991572847
(marginal [-1 0 1] 1.01) ; 3.379711207009579
(marginal [-1 0 1] 0.99) ; 3.3796886718908556


;; Hmm. Looks like it's 0.81 again, which it really shouldn't be!







