;; Occam's Razor

;; (Exercise 28.1 from Information Theory, Inference and Learning Algorithms, by David Mackay)

;; The old principle of choosing the simplest model to fit the facts is given
;; quantitative form in Bayes' Theorem.

;; Let's imagine that we're getting random samples, and we have two models which
;; may explain the data.

;; Here's an example data vector
(def data [ 0.1 0.3 0.5 0.7 0.9 ])

;; In the first model (H0), the chance of getting a number between -1 and 1 is uniformly
;; distributed. P(x|H0)=1/2 for all x

(defn h0[x] (if (<= -1 x 1) 1/2 0))

;; Then the probability of any data vector is given by multiplying together the
;; probabilities of each point according to the model

(defn probability [model data]
  (reduce * (map model data)))

;; For any five point data vector which is in the right range, h0 always gives
;; us a probability of 1/2^5. It's equally likely to generate all possible data
;; sets.

(defn evidenceh0 [data]
  (probability h0 data))

;; We'll call that number (the probability of the data given the model) the
;; evidence for H0:

(evidenceh0 data) ; 1/32

;; A more interesting model that might produce our data biases the probability
;; linearly. But the degree of bias is itself an adjustable parameter. Here's a
;; function which makes models according to a parameter m (which should be
;; between -1/2 and +1/2 to avoid getting funny results.

(defn h1 [m]
  (fn[x] (if (<= -1 x 1) (+ 1/2 (* m x)) 0)))

;; If m=0 then that's just the same model
((h1 0) 1) ; 1/2
;; But for m=1/4, say, larger values are more likely.
((h1 1/4) 1) ; 3/4

;; So the model H1 with m=1/2 is much better at explaining the observed data:
;; The evidence is ~ 1/5 rather than 1/32.

(probability (h1 1/2) data) ; 0.21651093750000003

;; But which model should we choose?

;; It seems intuitively rather dishonest to create a model with parameters, then
;; pick the parameters so that they fit the data, and then to declare that that model wins.

;; It's that sort of thinking that Occam's Razor warns us to avoid, and that's
;; why we believe that the planets go round the Sun according to the Copernican
;; model of ellipses rather than the older theory that the other planets and the
;; Sun go round the Earth in a complicated series of epicycles.

;; If anything, in Copernicus' time, the epicyclic model, once carefully tuned,
;; fit the observed data rather better than the heliocentric model. Copernicus
;; model won because it seems intuitively better, needing much less tuning and
;; coincidence to model the data.

;; Of course neither are actually correct. Even better models are given by
;; Newton's laws or Einsteing geometrodynamics.

;; How does Bayes tell us to choose?

;; Well, first of all, we need a prior for the parameter m.

;; We'll just say that it can take any value between -1/2 and 1/2 with equal probability.

;; Then we can calculate the 'evidence' for H1 and that uniform prior as the integral over
;; all the possible m, which we can approximate by:

(defn evidenceh1 [data]
  (* 0.01 (reduce + (map #(probability (h1 %) data) (range -1/2 1/2 0.01)))))

(evidenceh1 data) ; 0.05518837559685008

;; That's about 1/20, versus 1/32 for the simple model.
;; the extra explanatory ability of the model with the parameter is being compensated for
;; by the difficulty of choosing a parameter which makes it more likely.

(defn evidenceratio [data]
  (/ (evidenceh1 data)
     (evidenceh0 data)))

;; In this case, we can't really decide.

;; Suppose our adversary takes a random choice of models,
;; i.e. He tosses a coin to decide between H1 or H0, and then if he gets H1 choosing m
;; at random between -1/2 and 1/2.

;; If we see our data, it's about 17:10 that the model the adversary is using is H1

(evidenceratio [0.1 0.3 0.5 0.7 0.9]) ; 1.7660280190992026

;; On the other hand, if the data's spread out, the simplicity of the H0 model
;; makes it slightly more likely.
(evidenceratio [-1 -0.5 0 0.5 1.0]) ; 0.6332833319999994

;; If we see horribly unlikely (but possible) data like:

(evidenceratio [-1 -1 -1 -1 -1  1 1 1 1 1]) ; 0.36940836940025096

;; The H0 is about 3 times more likely to be true.

;; But if the data's all up one end of the range
;; Then it's much more likely that the data was generated by H1 with an appropriate parameter:

(evidenceratio [1 1 1 1 1 1 1 1 1 1]) ; 88.05623218526695






