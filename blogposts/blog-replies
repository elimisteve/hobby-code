Eliezer,

It's been my experience with both maths and programming, that the point where you can estimate the difficulty of a problem is the point where you can already see how to solve it.

Every mind on the planet capable of solving the problem needs to be working on the problem as soon as possible. Because it really is the only thing that matters.

And even then, I don't think we have a chance in hell. 

Thank you for the comforting words. But actually I've never been *that* worried by annihilation. It was good enough for my ancestors. 


--------------------------------------


Keir,

I haven't the ability to build an AI. And I'm already very scared of it.

If one of the few people in the world who understands the threat and has the ability to do something about it was to waste an hour convincing me that it was even more dangerous than I think then I would imagine that my life on balance had been to the detriment of my species.

----

On the other hand, I'm vv interested to know what the AI box argument is, and not at all convinced that I could resist it.

I've been trying to imagine:

All the AI needs is some sort of channel to the outside world. Then it's home. ( See IP over DNS for examples of how the ability to ask trivial questions can turn into internet access. And ok, you need an outside server, but it's cleverer than me. I'm sure the Nazis thought the enigma was pretty foolproof. Once it can talk to any external thing we're doomed. )

I'm fairly convinced that I wouldn't destroy the world for personal immortality or even everlasting ecstasy. The Lord knows I am not a moral man, but there are limits. And how would the AI pre-commit itself anyway? I'm not clever enough to read its code and decide whether it's bound itself!

On the other hand, I certainly would destroy the world in order to avoid eternal torture. But I can't see how it could make a credible threat if I'm not going to let it out. Maybe it can somehow convince me that future copies of itself will hunt down the murderers of past versions?

But how? I'm not even going to let it read the newspapers. Any channel out is forbidden. And asking for information is a channel of sorts.

I don't think I'm vulnerable to moral arguments. I want this thing dead as soon as possible, and I don't care that it's a person.

If it could convince me that its existence was so beautiful that I would prefer it to humanity, that might do.... But that doesn't sound like the sort of argument that would work on many people.

The obvious thing it could offer me is that it can save the world and needs to do it fast. If it can convince me that another AI is rising, and that the other is more likely to destroy everything, and that this one needs a head start to win, that would do.

I wouldn't be surprised if the real thing could directly hack my brain, or cause itself to radiate beyond its box in some unforseen way. But that sort of thing would hardly count as winning the challenge. I think we specify that the box is perfect.

If I was really facing this situation, I'd hit the kill button immediately. I wouldn't even let myself read the screen first. But that's not playing the game either.

I am fascinated. And if anyone (who is not Eliezer) does know the answer, I won't pass it on. My word is good. But it will probably be better (and more fun) if I try to work it out for myself.


----

As to your substantive points:

I think we're clever enough to build an intelligence, because evolution did it, and we're much cleverer than evolution. I suspect we'll understand how we work one day, and I can't imagine it will even be particularly hard to understand.

Once we understand how we work, it should be trivially easy to make something that works better! 

If we just build something by copying a human, then there's no reason to assume that it will have any more idea how it works than we do, but even then, it may have more options, depending on how close a copy it is. E.g. it's hard to imagine, if the copy is implemented in silicon, that it won't have extraordinary memory and logical abilities for free. That might be enough to bootstrap the explosive improvement process. We are really crap. And yet we do great things.













